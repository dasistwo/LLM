{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6ed5b",
   "metadata": {},
   "source": [
    "# Plotting the LLM parameters\n",
    "This code is a simple implementation of visualization of the weight and activation of a LLM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a9bcc2-a9c6-490f-881d-f6618046b6a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /scale/cal/home/jychoi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# %pip install -Uq transformers huggingface-hub torch datasets matplotlib numpy\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, SwitchTransformersModel\n",
    "from datasets import load_dataset\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from packages import toolbox\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630db28c",
   "metadata": {},
   "source": [
    "First, load the model you want. SwitchTransformers are not automatically selected with AutoModel, so you need to load it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8315c6cb-2952-4bf4-98aa-49c2b45c992d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd2709eb5ca4b9c86caeab58d2b01fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_id = \"/data/storage1/model/huggingface/opt/1.3b/\"\n",
    "# model_id = \"/data/storage1/model/huggingface/gemma/7b/\"\n",
    "model_id = \"/data/storage1/model/huggingface/llama-2/7b/\"\n",
    "# model_id = \"/data/storage1/model/huggingface/mixtral/\"\n",
    "# model_id = \"google/switch-base-8\"\n",
    "# model_id = \"OrionZheng/openmoe-8b\"\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if 'switch' in model_name:\n",
    "    from transformers import SwitchTransformersModel\n",
    "    model = SwitchTransformersModel.from_pretrained(\n",
    "        model_id,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16\n",
    "        )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map='cuda:1',\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a17b8",
   "metadata": {},
   "source": [
    "Let's check the size of the model and its structure before we start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c257374-2fce-4b72-a217-0fbcd18b2db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 12916.516MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolbox.print_model_size(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b10857",
   "metadata": {},
   "source": [
    "The get_layer_weights function is used to extract the weights of the model.\n",
    "You may want to flatten the array to visualize it.\n",
    "If `flatten` is not specified, each element of the dataframe is typed with 2D numpy array.\n",
    "Else, the array is flattened and typed with 1D numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9da62e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(model, SwitchTransformersModel):\n",
    "    # Switch transformers have a different structure\n",
    "    df = toolbox.get_layer_weights_ST(model=model)\n",
    "    df_list = df.apply(lambda x: np.concatenate([l for l in x]), axis=1).to_list()\n",
    "\n",
    "else:\n",
    "    if \"openmoe\" in model_name:\n",
    "        neglect_list = [\"embed_tokens\", \"input_layernorm\", \"post_attention_layernorm\", \"pre_extra_mlp_layernorm\",\n",
    "                        \"lm_head\", \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"norm\"]\n",
    "    else:\n",
    "        neglect_list = [\n",
    "            \"embed_tokens\",\n",
    "            \"input_layernorm\",\n",
    "            \"post_attention_layernorm\",\n",
    "            \"lm_head\",\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            # \"gate_proj\",\n",
    "            # \"up_proj\",\n",
    "            # \"down_proj\",\n",
    "        ]\n",
    "\n",
    "    # df = toolbox.get_layer_weights(\n",
    "    #     model=model, neglect_list=neglect_list, flatten = False\n",
    "    # )\n",
    "    embed_np = model.model.embed_tokens.weight.detach().cpu().numpy()\n",
    "    # df_list = df.apply(lambda x: np.concatenate([l for l in x]), axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.map(np.var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27bb31",
   "metadata": {},
   "source": [
    "This draws the variance of the weight of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd633c-f033-42ae-919b-5808b57a7477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply numpy variance for every element\n",
    "df = df.apply(np.var)\n",
    "df = df.melt(id_vars='index')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Variance of Weights')\n",
    "plt.xticks(np.arange(min(df[\"Layer\"]), max(df[\"Layer\"]) + 1, 1.0))\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Variance')\n",
    "sns.lineplot(data=df, x='Layer', y='Variance', hue='Projection', palette='tab10', markers=True, style='Projection')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8655be",
   "metadata": {},
   "source": [
    "## Plot the activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299dce8",
   "metadata": {},
   "source": [
    "This is for data preparation of the activation.\n",
    "`get_act_scales` is a function to get the activation scales of the model.\n",
    "It runs the model with `num_samples` and `seq_len` and track the activation scales of each layer. Please be aware that sequence length is default to 2048, and it may not work with a different sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e413e-86cf-460d-b5cd-8c1db1176395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is fixed with wikitext-2. change if needed.\n",
    "# Changed from https://github.com/mit-han-lab/smoothquant/blob/main/smoothquant/calibration.py\n",
    "\n",
    "def get_act_scales(model, tokenizer, seq_len=2048):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    act_scales = {}\n",
    "\n",
    "    def stat_tensor(name, tensor):\n",
    "        hidden_dim = tensor.shape[-1]\n",
    "        tensor = tensor.view(-1, hidden_dim).abs().detach()        \n",
    "        comming_max = torch.max(tensor, dim=0)[0].float().cpu()\n",
    "        if name in act_scales:\n",
    "            act_scales[name] = torch.max(act_scales[name], comming_max)\n",
    "        else:\n",
    "            act_scales[name] = comming_max\n",
    "\n",
    "    # def stat_tensor(name, tensor):\n",
    "    #   hidden_dim = tensor.shape[-1]\n",
    "    #   tensor = tensor.view(-1, hidden_dim).abs().detach()\n",
    "    #   comming_max = tensor.float().cpu()\n",
    "\n",
    "    #   if name in act_scales:\n",
    "    #       # compare the shape of two tensors\n",
    "    #       max_dim = max(act_scales[name].shape[0], comming_max.shape[0])\n",
    "\n",
    "    #       # extend the smaller tensor\n",
    "    #       act_scales[name] = torch.cat([act_scales[name], torch.zeros(max_dim - act_scales[name].shape[0], hidden_dim)], dim=0)\n",
    "    #       comming_max = torch.cat([comming_max, torch.zeros(max_dim - comming_max.shape[0], hidden_dim)], dim=0)\n",
    "\n",
    "    #       # get the max value\n",
    "    #       act_scales[name] = torch.max(act_scales[name], comming_max)\n",
    "    #   else:\n",
    "    #       act_scales[name] = comming_max\n",
    "\n",
    "    def stat_input_hook(m, x, y, name):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        stat_tensor(name, x)\n",
    "\n",
    "    hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(\n",
    "                    functools.partial(stat_input_hook, name=name))\n",
    "            )\n",
    "\n",
    "    dataset = load_dataset(path=\"wikitext\", name=\"wikitext-2-v1\", split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=944)\n",
    "    encodings = tokenizer(\n",
    "        \"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\", truncation=True\n",
    "    ).input_ids\n",
    "    num_samples = encodings.numel() // seq_len\n",
    "\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        input_ids = encodings[:, i * seq_len : (i+1) * seq_len].to(device)\n",
    "        model(input_ids)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return act_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523f3ab-540a-490d-817c-02d259498c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file if it exists\n",
    "try:\n",
    "    act_scales = torch.load(f\"../act_scales_{model_name}.pt\")\n",
    "except FileNotFoundError:\n",
    "    # Get the scales of the input matrices\n",
    "    act_scales = get_act_scales(model, tokenizer, seq_len=2048)\n",
    "    # Save act_scales as a loadable file.\n",
    "    torch.save(act_scales, f\"../act_scales_{model_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1287237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove the lm_head from the list\n",
    "if \"lm_head\" in act_scales.keys():\n",
    "    act_scales.pop(\"lm_head\")\n",
    "\n",
    "keys_list = list(act_scales.keys())\n",
    "\n",
    "# get the number of layers\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.ModuleList):\n",
    "       layerlist = module\n",
    "       \n",
    "try:\n",
    "    layer_count = len(layerlist)\n",
    "except NameError:\n",
    "    raise ValueError(\"layerlist is not defined. Please check the model structure.\")\n",
    "\n",
    "# get unit and projection names\n",
    "unit_projections = {}\n",
    "for key in keys_list:\n",
    "    unit = key.split(\".\")[-2]\n",
    "    projection = key.split(\".\")[-1]\n",
    "    if unit not in unit_projections:\n",
    "        unit_projections[unit] = []\n",
    "    if projection not in unit_projections[unit]:\n",
    "        unit_projections[unit].append(projection)\n",
    "\n",
    "proj_count = sum(len(v) for v in unit_projections.values())\n",
    "proj_list = sum(unit_projections.values(), [])\n",
    "print(unit_projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=df_list, whis=(0, 100), palette=[\"blue\", \"orange\"])  # type: ignore\n",
    "\n",
    "plt.title(f\"Boxplots for {model_name} layer weights\")\n",
    "plt.xlabel(\"Layers\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.savefig(f\"../plots/{model_name}_boxplot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2392acb9",
   "metadata": {},
   "source": [
    "It is quite meaningless to see the variance of the weight of the LLM model, so let's try the activation of the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab90155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "\n",
    "# # Loop through the act_scales dictionary. key is the name of the layer and value is the scale of the input matrix\n",
    "# for name, scale in act_scales.items():\n",
    "#     for units, projections in unit_projections.items():\n",
    "#         unit = getattr(layer, units)\n",
    "#         for projection in projections:\n",
    "#             weight = getattr(unit, projection).weight.detach().cpu().numpy()\n",
    "#             variance = np.var(weight)\n",
    "#             data.append({\"Layer\": int(name.split(\".\")[2]), \"Variance\": variance, \"Projection\": projection})\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Layer\", \"Variance\", \"Projection\"])\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.title('Variance of Weights')\n",
    "plt.xticks(np.arange(min(df[\"Layer\"]), max(df[\"Layer\"]) + 1, 1.0))\n",
    "sns.lineplot(data=df, x='Layer', y='Variance', hue='Projection', palette='tab10', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e61c5-cdf1-4aaf-b54a-a43107022fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 24 subplots in a 24x6 grid with larger size\n",
    "fig = plt.figure(figsize=(layer_count*4, proj_count*4))\n",
    "\n",
    "# Initialize global minimum and maximum Z values\n",
    "global_z_min = np.inf\n",
    "global_z_max = -np.inf\n",
    "\n",
    "# Loop through the layer and channel names\n",
    "for i, (name, scale) in enumerate(act_scales.items()):\n",
    "  # Get the layer and channel indices\n",
    "  parts = name.split(\".\")\n",
    "  # print(parts)\n",
    "  if parts[-1] not in proj_list:\n",
    "    continue\n",
    "  layer = int(parts[2])\n",
    "  channel = parts[-1]\n",
    "  \n",
    "  # Create a subplot with 3D projection\n",
    "  ax = fig.add_subplot(len(model.model.layers), len(proj_list), i+1, projection='3d')\n",
    "      \n",
    "  # Get X and Y coordinates from scale tensor shape\n",
    "  X = np.arange(scale.shape[1]) # first dimension\n",
    "  Y = np.arange(scale.shape[0]) # second dimension\n",
    "  \n",
    "  # Create a mesh grid for X and Y\n",
    "  X, Y = np.meshgrid(X, Y)\n",
    "  \n",
    "  # Get Z values from scale tensor values\n",
    "  Z = scale.numpy() # convert tensor to numpy array\n",
    "  \n",
    "  # Update global minimum and maximum Z values\n",
    "  global_z_min = min(global_z_min, Z.min())\n",
    "  global_z_max = max(global_z_max, Z.max())\n",
    "  \n",
    "  # Plot surface plot with colormap\n",
    "  ax.plot_surface(X, Y, Z, cmap='seismic')\n",
    "\n",
    "  # Set the subplot title with smaller font size\n",
    "  ax.set_title(f\"Layer {layer}, {parts[-1]}\", fontsize=12)\n",
    "  # Calculate the range of the data\n",
    "  rng = scale.max() - scale.min()\n",
    "  \n",
    "  # Add a caption with the range value under the plot\n",
    "  ax.text2D(0.5, 1, f\"range = {rng:.2f}\", horizontalalignment=\"center\", verticalalignment=\"top\", transform=ax.transAxes, fontsize=12)\n",
    "\n",
    "# Loop through the axes again to set the same Z limit for all plots\n",
    "for ax in fig.get_axes():\n",
    "  ax.set_zlim(global_z_min, global_z_max)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{model_name}_act.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 24 subplots in a 24x6 grid with larger size\n",
    "fig = plt.figure(figsize=(layer_count * 4, proj_count * 4))\n",
    "\n",
    "# Initialize global minimum and maximum Z values\n",
    "global_z_min = np.inf\n",
    "global_z_max = -np.inf\n",
    "\n",
    "# Loop through the layer and channel names\n",
    "for i, (name, scale) in enumerate(act_scales.items()):\n",
    "    # Get the layer and channel indices\n",
    "    parts = name.split(\".\")\n",
    "    # print(parts)\n",
    "    if parts[-1] not in proj_list:\n",
    "        continue\n",
    "    layer = int(parts[2])\n",
    "    channel = parts[-1]\n",
    "\n",
    "    # Create a subplot with 3D projection\n",
    "    ax = fig.add_subplot(\n",
    "        len(model.model.layers), len(proj_list), i + 1, projection=\"3d\"\n",
    "    )\n",
    "\n",
    "    # Get X and Y coordinates from scale tensor shape\n",
    "    X = np.arange(scale.shape[1])  # first dimension\n",
    "    Y = np.arange(scale.shape[0])  # second dimension\n",
    "\n",
    "    # Create a mesh grid for X and Y\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "    # Get Z values from scale tensor values\n",
    "    Z = scale.numpy()  # convert tensor to numpy array\n",
    "\n",
    "    # Update global minimum and maximum Z values\n",
    "    global_z_min = min(global_z_min, Z.min())\n",
    "    global_z_max = max(global_z_max, Z.max())\n",
    "\n",
    "    # Plot surface plot with colormap\n",
    "    ax.plot_surface(X, Y, Z, cmap=\"seismic\")\n",
    "\n",
    "    # Set the subplot title with smaller font size\n",
    "    ax.set_title(f\"Layer {layer}, {parts[-1]}\", fontsize=12)\n",
    "    # Calculate the range of the data\n",
    "    rng = scale.max() - scale.min()\n",
    "\n",
    "    # Add a caption with the range value under the plot\n",
    "    ax.text2D(\n",
    "        0.5,\n",
    "        1,\n",
    "        f\"range = {rng:.2f}\",\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"top\",\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "# Loop through the axes again to set the same Z limit for all plots\n",
    "for ax in fig.get_axes():\n",
    "    ax.set_zlim(global_z_min, global_z_max)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{model_name}_act.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
