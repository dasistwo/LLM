{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from awq.evaluation import evaluate_perplexity as Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the perplexity of the model on the test set.\n",
    "FP16, INT4-awq, INT4-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838bcb1792734f3bb1d3f600ff7bfc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jychoi/.conda/envs/hf/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533c31f2031e42f59e15c93e295c9dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 40/40 [00:08<00:00,  4.66it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f40404f531e4952ae769d14e441e7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/923 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers...: 100%|██████████| 40/40 [00:01<00:00, 26.27it/s]\n",
      "/home/jychoi/.conda/envs/hf/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "2024-06-28:14:29:23,376 INFO     [_base.py:922] You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "2024-06-28:14:29:23,829 INFO     [_base.py:1011] The layer lm_head is not quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c34e4444097489a9a45b4c352c5b515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1523 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28:14:29:29,380 WARNING  [modeling.py:1812] Some weights of the model checkpoint at /scale/cal/home/jychoi/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-GPTQ/snapshots/dc078861968fc39b3303df619d1606c7528c6f87/model.safetensors were not used when initializing LlamaForCausalLM: {'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq'}. This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n",
      "Perplexity 4.883: 100%|██████████| 166/166 [1:23:44<00:00, 30.27s/it]\n",
      "Perplexity 4.971: 100%|██████████| 166/166 [01:38<00:00,  1.68it/s]\n",
      "Perplexity 4.973: 100%|██████████| 166/166 [01:06<00:00,  2.51it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb202f7347c4a599750358ed4971272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f733788f814a7aaa760c86d8e1dc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62c7dddfb8b40c394fa1e915cce9b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/739 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers...: 100%|██████████| 32/32 [00:02<00:00, 14.44it/s]\n",
      "/home/jychoi/.conda/envs/hf/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "2024-06-28:15:56:51,541 INFO     [_base.py:922] You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "2024-06-28:15:56:51,977 INFO     [_base.py:1011] The layer lm_head is not quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa76166c8614b48b7da6ec4804706ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1219 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28:15:57:01,285 WARNING  [modeling.py:1812] Some weights of the model checkpoint at /scale/cal/home/jychoi/.cache/huggingface/hub/models--TheBloke--Llama-2-7B-GPTQ/snapshots/60b5c75d47a7d925782e74d16b6686cf0abbd052/model.safetensors were not used when initializing LlamaForCausalLM: {'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq'}. This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n",
      "Perplexity 5.472: 100%|██████████| 166/166 [45:33<00:00, 16.47s/it]\n",
      "Perplexity 5.607: 100%|██████████| 166/166 [00:57<00:00,  2.91it/s]\n",
      "Perplexity 5.730: 100%|██████████| 166/166 [00:39<00:00,  4.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for size in [\n",
    "    \"3-8B\",\n",
    "    \"2-7B\",\n",
    "    \"2-13B\",\n",
    "]:\n",
    "    fp16_model_id = \"/data/storage1/model/huggingface/llama/{}\".format(size)\n",
    "    gptq_model_id = \"TheBloke/Llama-{}-GPTQ\".format(size)\n",
    "    awq_model_id = \"TheBloke/Llama-{}-AWQ\".format(size)\n",
    "    if size == \"3-8B\":\n",
    "        gptq_model_id = \"TechxGenus/Meta-Llama-3-8B-GPTQ\"\n",
    "        awq_model_id = \"TechxGenus/Meta-Llama-3-8B-AWQ\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(fp16_model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(fp16_model_id)\n",
    "    awq_model = AutoAWQForCausalLM.from_quantized(awq_model_id)\n",
    "    gptq_model = AutoGPTQForCausalLM.from_quantized(gptq_model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "    for testcases in [model, awq_model.model, gptq_model]:\n",
    "        Perplexity(testcases, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.1226: 100%|██████████| 163/163 [1:00:30<00:00, 22.27s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea8f8dc3dab491fbad6804051acbe54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.2486: 100%|██████████| 163/163 [01:45<00:00,  1.55it/s]\n",
      "Perplexity: 6.6099: 100%|██████████| 163/163 [01:28<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# This case is not reliable than AWQ Perplexity method.\n",
    "from auto_gptq.utils import Perplexity as GPTQPerplexity\n",
    "for size in [\n",
    "    \"3-8B\",\n",
    "    \"2-7B\",\n",
    "    \"2-13B\",\n",
    "]:\n",
    "    fp16_model_id = \"/data/storage1/model/huggingface/llama/{}\".format(size)\n",
    "    gptq_model_id = \"TheBloke/Llama-{}-GPTQ\".format(size)\n",
    "    awq_model_id = \"TheBloke/Llama-{}-AWQ\".format(size)\n",
    "    if size == \"3-8B\":\n",
    "        gptq_model_id = \"TechxGenus/Meta-Llama-3-8B-GPTQ\"\n",
    "        awq_model_id = \"TechxGenus/Meta-Llama-3-8B-AWQ\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(fp16_model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(fp16_model_id)\n",
    "    awq_model = AutoAWQForCausalLM.from_quantized(awq_model_id)\n",
    "    gptq_model = AutoGPTQForCausalLM.from_quantized(gptq_model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "    for testcases in [model, awq_model.model, gptq_model]:\n",
    "        ppl = GPTQPerplexity(testcases, tokenizer)\n",
    "        ppl.calculate_perplexity(2048, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"--------------\")\n",
    "for name, data in awq_model.model.model.layers[0].attn.o_proj.named_buffers():\n",
    "    print(name, data.nbytes)\n",
    "print(\"--------------\")\n",
    "for name, data in gptq_model.model.model.layers[0].self_attn.o_proj.named_buffers():\n",
    "    print(name, data.nbytes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
